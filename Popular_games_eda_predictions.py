# -*- coding: utf-8 -*-
"""Popular_games_EDA_Predictions_API.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MHpLaladahEsbqR2pnmqoxoIyZCVbEUD

### Preparation
"""

# import basic libraries
import pandas as pd
import numpy as np
import seaborn as sns
import numpy as np

import matplotlib.pyplot as plt

# libraries for models
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

from keras.models import Sequential
from keras.layers import Dense

# additional tools
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/games.csv')

df.sample(2)

df.info()

# seems like we have a lot of improper types
# let's fix them throughout preparation

# looking through nulls
null_count = df.isnull().sum()
null_count

null_count[null_count != 0]

"""### Data pre-processing"""

df['Release Date'] = pd.to_datetime(df['Release Date'])

# we have an error while converting the date to date type
# we can try to get rid of the releases on TBD

cond = df["Release Date"] == "releases on TBD"
df[cond]

# let's just drop them, as there are only three variables
df = df.drop([644, 649, 1252], axis=0)

# we can additionally drop other columns, because here we don't need them
df = df.drop(columns=['Unnamed: 0', 'Times Listed', 'Summary', 'Reviews', 'Team'])

# make all the column names lowercase
df.columns = df.columns.str.lower()

# replace spaces with _
df.columns = df.columns.str.replace(' ', '_')

rename_mapper = {"plays":"no_of_plays", "playing":"active_players"}
df.rename(columns=rename_mapper, inplace=True)
df.head(2)

def convert_str_num_to_int(df: pd.DataFrame, col_names: list) -> pd.DataFrame:
   # taking a copy of the provided dataframe
    new_df = df.copy()

    # looping over the column names list
    for col in col_names:
        # replace 'K' with '000'
        new_df[col] = new_df[col].str.replace("K", "000")

        # multiply the values by 1000 to handle decimal values correctly
        new_df[col] = new_df[col].map(lambda x: int(float(x) * 1000) if '.' in x else int(x))

    return new_df

# converting the columns to numeric types
numerical_cols = ["no_of_plays", "active_players", "wishlist", 'number_of_reviews', 'backlogs']
df = convert_str_num_to_int(df, numerical_cols)
df.head(2)

# checking the types
df.dtypes

# dropping nulls
df.dropna(inplace=True)

df.head(2)

# calculate correlation between columns
corr = df.corr()

# generate a heatmap visualization of the correlation matrix
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap='RdBu', annot=True, square=True)

"""Here, it seems we have somewhat strong correlation b/w backlogs and wishlist, number of reviews and wishlist, number of reviews and backlogs."""

# we can save dataframe for visualization with dashboards
df.to_csv('/content/drive/My Drive/popular_games.csv')

# replacing '[', ']', and "'" characters in the genres column with nothing
df["genres"] = df["genres"].str.replace('[\[\]\'\"]', "")

# creating a binary matrix of genre values for each game
genres = df["genres"].str.get_dummies(",")

# summing the occurrences of each genre across all games
popularity = genres.sum().sort_values(ascending=False)

# printing the popularity of each genre
popularity.head()

# converting the popularity to a dataframe
popularity_df = popularity.to_frame(name="popularity")

# merging the popularity data with the game data
df_pop = df.merge(popularity_df, left_on="genres", right_index=True)

# grouping the data by genre
grouped = df_pop.groupby("genres")

df_pop.head(5)

# creating a csv file for genres for visualization
df_pop.to_csv('/content/drive/My Drive/genre_popularity.csv')

# which game is the most popular for each genre
def get_most_pop_game(group: pd.DataFrame) -> pd.Series:

   # getting the row with the highest popularity value
    most_popular_game = group.loc[group["popularity"].idxmax()]

    # returning the title of the most popular game
    return most_popular_game["title"]

# applying the function to the grouped dataframe and saving the result (series) into a variable
most_popular_game = grouped.apply(get_most_pop_game)

# converting the series into a dataframe, reseting the index and rename the columns
# to present the most popular game in each video game genre.
most_popular_game.to_frame().reset_index() \
                 .rename({"genres":"Game Genre", 0:"Most Popular Game"}, axis=1)

"""### Models"""

# let's first take all the features we have
features = ['number_of_reviews', 'no_of_plays', 'active_players', 'backlogs', 'wishlist', 'popularity']
x = df_pop.loc[:, features]
y = df_pop.loc[:, ['rating']]

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, train_size=0.75)

"""#### Random Forest"""

# define the parameter grid for random search
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2'],
    'bootstrap': [True, False]
}

# create the RandomForestRegressor model
model = RandomForestRegressor(random_state=0)

# perform random search with error_score='raise'
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,
                                   scoring='neg_mean_squared_error', cv=5,
                                   random_state=0, n_iter=10, error_score='raise')

# fit the random search to the training data
random_search.fit(x_train, y_train)

# get the best model from the random search
best_model = random_search.best_estimator_

# make predictions on the test data
y_pred = best_model.predict(x_test)

# evaluate the model using mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error: ", mse)
r2 = r2_score(y_test, y_pred)
print("R2 Score: ", r2)

"""**Random Forest with Random Search**

Mean Squared Error:  0.18988211575937827

R2 Score:  0.3204534030420525

#### XGBoost
"""

# define the parameter grid for random search
param_grid = {
    'n_estimators': randint(100, 1000),
    'max_depth': randint(1, 10),
    'learning_rate': uniform(0.01, 0.5),
    'subsample': uniform(0.5, 0.5),
    'colsample_bytree': uniform(0.5, 0.5),
    'gamma': uniform(0, 0.5)
}

# create the XGBoost regressor
model = XGBRegressor(objective='reg:squarederror')

# perform random search
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,
                                   scoring='neg_mean_squared_error', cv=5,
                                   random_state=0, n_iter=10, error_score='raise')

# fit the random search to the training data
random_search.fit(x_train, y_train)

# get the best model from random search
best_model = random_search.best_estimator_

# make predictions on the test data
y_pred = best_model.predict(x_test)

# evaluate the model using R2 score, MAE, and RMSE
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("R2 Score: ", r2)
print("Mean Absolute Error: ", mae)
print("Root Mean Squared Error: ", rmse)

"""**XGBoost**

R2 Score:  0.2948723189187711

Mean Absolute Error:  0.3206812422040482

Root Mean Squared Error:  0.44388073055632177

## Change the model

As we have low values for our models, we can try to lower the number of features and take only those meaningful for our target variable.
"""

# calculate correlation between columns
corr = df_pop.corr()

# generate a heatmap visualization of the correlation matrix
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap='RdBu', annot=True, square=True)

df_pop.head(2)

df_pop1 = df_pop.drop(columns=['title', 'release_date', 'genres'])
df_pop1.head(1)

"""Rating is our target variable

We can take number_of_reviews, backlogs, and wishlist as meaningful features.

"""

df_pop1 = df_pop1.drop(columns=['active_players', 'no_of_plays', 'popularity'])

fig, axs = plt.subplots(1, 3, figsize=(15, 5))

axs[0].scatter(df_pop1['number_of_reviews'], df_pop1['rating'])
axs[0].set_xlabel('Number of Reviews')
axs[0].set_ylabel('Rating')

axs[1].scatter(df_pop1['backlogs'], df_pop1['rating'])
axs[1].set_xlabel('Backlogs')
axs[1].set_ylabel('Rating')

axs[2].scatter(df_pop1['wishlist'], df_pop1['rating'])
axs[2].set_xlabel('Wishlist')
axs[2].set_ylabel('Rating')

plt.tight_layout()
plt.show()

"""The features are positively correlated."""

features = ['number_of_reviews', 'backlogs', 'wishlist']
x = df_pop1.loc[:, features]
y = df_pop1.loc[:, ['rating']]

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, train_size=0.75)

# define the parameter grid for random search
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2'],
    'bootstrap': [True, False]
}

# create the RandomForestRegressor model
model = RandomForestRegressor(random_state=0)

# perform random search with error_score='raise'
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,
                                   scoring='neg_mean_squared_error', cv=5,
                                   random_state=0, n_iter=10, error_score='raise')

# fit the random search to the training data
random_search.fit(x_train, y_train)

# get the best model from the random search
best_model = random_search.best_estimator_

# make predictions on the test data
y_pred = best_model.predict(x_test)

# evaluate the model using mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error: ", mse)
r2 = r2_score(y_test, y_pred)
print("R2 Score: ", r2)

"""**Random Forest**

Mean Squared Error:  0.17040259040261457

R2 Score:  0.3901663673915716

### Logarithmic transformation

As our features of the same scale but of different with the target variable, we can try to take a log of them.
"""

# define the features and target variable
features = ['number_of_reviews', 'backlogs', 'wishlist']
x = df_pop1.loc[:, features]
y = df_pop1.loc[:, 'rating']

# take the logarithm of the selected features
x['number_of_reviews'] = np.log(x['number_of_reviews'])
x['backlogs'] = np.log(x['backlogs'])
x['wishlist'] = np.log(x['wishlist'])

# define the parameter grid for random search
param_grid = {
    'n_estimators': randint(100, 1000),
    'max_depth': randint(1, 10),
    'learning_rate': uniform(0.01, 0.5),
    'subsample': uniform(0.5, 0.5),
    'colsample_bytree': uniform(0.5, 0.5),
    'gamma': uniform(0, 0.5)
}

# create the XGBoost regressor
model = XGBRegressor(objective='reg:squarederror')

# perform random search
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,
                                   scoring='neg_mean_squared_error', cv=5,
                                   random_state=0, n_iter=10, error_score='raise')

# fit the random search to the training data
random_search.fit(x, y)

# get the best model from random search
best_model = random_search.best_estimator_

# make predictions on the test data
y_pred = best_model.predict(x)

# evaluate the model using R2 score, MAE, and RMSE
r2 = r2_score(y, y_pred)
mae = mean_absolute_error(y, y_pred)
rmse = mean_squared_error(y, y_pred, squared=False)
print("R2 Score: ", r2)
print("Mean Absolute Error: ", mae)
print("Root Mean Squared Error: ", rmse)

"""R2 Score:  0.8297403567088253

Mean Absolute Error:  0.16734585745114805

Root Mean Squared Error:  0.2091436874778894
"""